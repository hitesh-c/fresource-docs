## Chapter 2.1: Multithreaded Programming (MP)

- Benefits of multithreaded programming
	+ Responsiveness - user experience improved
	+ Resource Sharing - share of code and data
	+ Economy - no need to create process all over again
	+ Scalability - can take advantage of multiprocessor environment

- > MP provides a mechanism for more efficient use of these multiple computing cores and improved concurrency; consider an application with four threads. on a system with single computing core, concurrency merely means that the execution of the threads will be interleaved, because the processing core is capable of executing only one thread at a time. On a system with multiple cores, concurrency means that the threads can run in parallel, because the system can assign a separate thread to each core. 


- In multicore environment threads can run in parallel.

- > A system is parallel if it can perform more than one task simultaneously. In contrast, a concurrent system supports more than one task by allowing all tasks to make progress. Thus, it is possible to have concurrency without parallelism. CPU schedulers were designed to provide the illusion of parralelism by rapidly switching between processes in the system, thereby allowing each process to make progress (such processes were running concurrently, but not in parallel).


- Areas of challenge when programming for multicore systems:	+ Identifying tasks - tasks that can run independent of one another
	+ Balance -  tasks should perform equal work for efficient use of cores
	+ Data splitting - 
	+ Data dependency
	+ Testing and debugging

- Two type of parallelism: data parallelism and task parallelism

- Two type of threads: user threads and kernel threads

- Many to one model (many user threads to one kernel threads); doesn't take advantage of multiple processing cores; multiple threads are unable to run in parallel.


- One to one model (one user thread to one kernel thread); overhead in the creation of kernel thread; support parallelism

- Many to many mode (developers can create as many user threads as necessary, and the corresponding kernel threads can run in parallel)

- **Synchronous threading** - occurs when the parent thread creates one or more children and then must wait for all its children to terminate before it resumes - fork-join strategy; lots of data sharing.

- **Asynchronous threading** - once the parent creates a child thread, the parent resumes execution, so that the parent and child can execute concurrently; little data sharing

- **Immutable** - once value it set, it cannot be changed.

- Implicit threading - creating and managing thread is transfered from developer to compilers and run-time libraries.

- Three type of implicit threading: 
	+ Thread pools - dealing with problem of unlimited threads (these can exhaust system resources, such as CPU time or memory); it entails creating multiple threads on process startup which are to be used when a request is made; avoids waiting for new threads to be created; limits the amount of threads created;
	+ OpenMP
	+ Grand Central Dispatch (GCD); iOS

- Parallel region - blocks of code that may run in parallel

- Signal - indicate that an event has occurred.

- Synchronous signal - signals are delivered to that same process that performed the operation that caused the signal (e.g., illegal memory access and division by 0).

- Asynchronous signal - signals are generated by an event external to a running process (e.g., CTRL-C).

- Default signal handler is when the signal is handled by kernel; user-defined signal handler is when a user overrides the handler.

- Thread Cancellation terminates **target threads** in two ways:
	+ Asynchronous cancellation - one thread immediately terminates target thread; can lead to resources not freed up due to canceling of a thread. 
	+ Deferred cancellation - A target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion; safer canceling of threads (avoids any unexpected termination or cancellation of threads.)


-  Threads belonging to a process share the data of the process; In some cases, each thread might need its own copy of data, which is called **Thread local storage (TLS)**.

- Lightweight process (LWP) a data structure between the user thread and kernel thread. Serves as a virtual processor in which the user thread can run. This is also known as the scheduler activation, where communication between the user thread library and the kernel happens; 

- **Upcall** when the kernel must inform an application about certain events. Upcalls are handled by the thread library with an upcall handler, which runs on a virtual processor.